{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XvMlNQwb8PtU",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Hello World in Deep Learning (MNIST Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cd7dh5Cd8PtW",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import datetime, os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tensorflow.keras.models import Sequential, Model, clone_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Input\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HPfZiH8IvetN",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Mnist classification in a nutshell\n",
    "\n",
    "In the next cell we train a simple fully connected neutal network to classify digits (0-9) form the mnist dataset. We use 20'000 images as our train dataset and 10'000 images are in our testset. Finally we plot the learning cuves and look at some predictions and the accuracy on the testset, you see that we already have an accuracy of around 96%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zmWTHT_MvetO",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "(x_digits_train, y_digits_train), (x_digits_test, y_digits_test) = mnist.load_data()\n",
    "\n",
    "# Make train data smaller\n",
    "np.random.seed(72)\n",
    "train_data_idx=np.random.choice(range(0,len(x_digits_train)),20000,replace=False)\n",
    "x_digits_train=x_digits_train[train_data_idx]\n",
    "y_digits_train=y_digits_train[train_data_idx]\n",
    "\n",
    "# Preprocess data \n",
    "x_digits_train = x_digits_train.astype('float32') \n",
    "x_digits_test = x_digits_test.astype('float32')\n",
    "x_digits_train = x_digits_train/ 255 \n",
    "x_digits_test = x_digits_test/ 255\n",
    "y_digits_train = to_categorical(y_digits_train, 10) \n",
    "y_digits_test = to_categorical(y_digits_test, 10)\n",
    "x_digits_train=x_digits_train.reshape((len(x_digits_train),28,28,1))\n",
    "x_digits_test=x_digits_test.reshape((len(x_digits_test),28,28,1))\n",
    "\n",
    "# Define model \n",
    "model_digits = Sequential()\n",
    "model_digits.add(Input(shape=(28,28,1)))\n",
    "model_digits.add(Flatten()) \n",
    "model_digits.add(Dense(500, activation='relu')) \n",
    "model_digits.add(Dense(50, activation='relu')) \n",
    "model_digits.add(Dense(10, activation='softmax')) \n",
    "\n",
    "# Compile model\n",
    "model_digits.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# train model\n",
    "history=model_digits.fit(x_digits_train, y_digits_train,\n",
    "                         validation_data=(x_digits_test, y_digits_test),\n",
    "                         batch_size=128, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QnJ-Wwo4vetU",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.figure(figsize=(14,6))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history.history['accuracy']) \n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='lower right')\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history.history['loss']) \n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ADP5P-GNveta",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# prediction of an image of the test set\n",
    "i=np.random.choice(range(0,len(x_digits_test))) \n",
    "plt.imshow(x_digits_test[i,:,:,0],cmap=\"gray\")\n",
    "pred=model_digits.predict(x_digits_test[i:i+1]) \n",
    "print(\"predicted probabilities\",pred)\n",
    "print(\"max probability\",np.max(pred))\n",
    "print(\"predicted label\",np.argmax(pred))\n",
    "print(\"true label\",np.argmax(y_digits_test[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AxJ1zbNavete",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# evaluation on the test set\n",
    "model_digits.evaluate(x_digits_test,y_digits_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SkOagcnPxnRm",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Mnist classification in more detail\n",
    "\n",
    "Now let's look at the code above in more detail.\n",
    "First we load the mnist dataset and look at the size of the train and test dataset. We have 60'000 train images and 10'000 test iamges. The images are greyscale images and the size is 28x28 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZuRrOHBJvetp",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Load pre-shuffled MNIST data into train and test sets\n",
    "(x_digits_train, y_digits_train), (x_digits_test, y_digits_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VYaxxufRvets",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(x_digits_train.shape)\n",
    "print(x_digits_test.shape)\n",
    "\n",
    "print(y_digits_train.shape)\n",
    "print(y_digits_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k469qlOwvetv",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In the next few cells we make the train dataset smaller by sampling 10'000 random images of the 60'000. We look at the distribution of the labels in both datasets and you can see that both dataset are more or less balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0wvn1u2yvetv",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(72)\n",
    "train_data_idx=np.random.choice(range(0,len(x_digits_test)),10000,replace=False)\n",
    "x_digits_train=x_digits_train[train_data_idx]\n",
    "y_digits_train=y_digits_train[train_data_idx]\n",
    "print(x_digits_train.shape)\n",
    "print(y_digits_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8jG8lOT3vety",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.unique(y_digits_train,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0tsCR-8Zvet1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.unique(y_digits_test,return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WQtLY5Ysvet5",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's look at the pixelvalues of a train image, you can see that the values are between 0 and 255.  We normalize the values to be in the range from 0 to 1, by values with 255. If you look at the labels, you see that the lables are values form 0 to 9, to train a neural network we need to transform it to the so called one hot encoding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YatbGqsBvet6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#print the pixel values of the first \"image\"\n",
    "print(x_digits_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mFzZTxTTvet-",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#print the label of the first \"image\"\n",
    "print(y_digits_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zaZ8_qmJveuD",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Preprocess data (normalize to be in the range [0,1])\n",
    "x_digits_train = x_digits_train.astype('float32')\n",
    "x_digits_test = x_digits_test.astype('float32')\n",
    "x_digits_train = x_digits_train/ 255\n",
    "x_digits_test = x_digits_test/ 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nNCv7CzaveuI",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Preprocess class labels -- one hot encoding\n",
    "y_digits_train = to_categorical(y_digits_train, 10)\n",
    "y_digits_test = to_categorical(y_digits_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3twMWtzRveuK",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#print the pixel values of the first \"image\"\n",
    "# now the values are form 0 to 1\n",
    "print(x_digits_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8n151fwLveuO",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#print the label of the first \"image\"\n",
    "#the 2 form above, one hot encoded\n",
    "print(y_digits_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "61-YE2EWveuR",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(x_digits_train.shape)\n",
    "print(x_digits_test.shape)\n",
    "\n",
    "print(y_digits_train.shape)\n",
    "print(y_digits_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vn7t82OrveuT",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's plot a few images to get a feeling for the dataset and see how hard the task is.\n",
    "We plot the first 9 images of the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nYtbsoc_8Ptf",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "for i in range(0,9):\n",
    "    sample_img = x_digits_train[i];\n",
    "    # plot the image\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.imshow(sample_img,cmap=\"gray\")\n",
    "    plt.title (\"Label: %s\"%np.where(y_digits_train[i]));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "emh5WWoUveuV",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In the next few cells we reshape the train and test dataset to be a 4 dim array. We have grayscale images and only one channel so we add one channel in the last dim. We define a neural network with keras, it has two fully connected layers with 500 and 50 nodes with the relu activation function. The last layer has 10 nodes and the softmax activation function, so we can interpret the output as probability for the predicted label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LndzoQD0veuX",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x_digits_train=x_digits_train.reshape((len(x_digits_train),28,28,1))\n",
    "x_digits_test=x_digits_test.reshape((len(x_digits_test),28,28,1))\n",
    "\n",
    "print(x_digits_train.shape)\n",
    "print(x_digits_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WfEANXC68Ptl",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define model architecture\n",
    "model_digits = Sequential()\n",
    "\n",
    "model_digits.add(Input(shape=(28,28,1)))\n",
    "model_digits.add(Flatten())\n",
    "model_digits.add(Dense(500, activation='relu'))\n",
    "model_digits.add(Dense(50, activation='relu'))\n",
    "model_digits.add(Dense(10, activation='softmax'))\n",
    " \n",
    "# Compile model\n",
    "model_digits.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RsFYPPgNveub",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_digits.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2rPuu731veue",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In the next two cells we evaluate the untrained model. As you can see the predictions of the untrained model are completely random, we have an accuracy of around 10%. If you look at single image preditions, you see that the predictions are random and wrong for most of the time. This will change when we train the model with our training dataset. To visualize the trainig process, the computational graph and development of the weights you can use Tensorboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rIVOjH4Qveuf",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# evaluation of the untrained model\n",
    "model_digits.evaluate(x_digits_test,y_digits_test)\n",
    "# you get the loss \"categorical_crossentropy\" and the accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FJEn1HNJveui",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# prediction of an image with the untrained model\n",
    "i=np.random.choice(range(0,len(x_digits_test)))\n",
    "plt.imshow(x_digits_test[i,:,:,0],cmap=\"gray\")\n",
    "pred=model_digits.predict(x_digits_test[i:i+1])\n",
    "print(\"predicted probabilities\",pred)\n",
    "print(\"max probability\",np.max(pred))\n",
    "print(\"predicted label\",np.argmax(pred))\n",
    "print(\"true label\",np.argmax(y_digits_test[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IU6IJQSM4Ik5",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lYctOEmR8Pto",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "\n",
    "# train the model\n",
    "history=model_digits.fit(x_digits_train, y_digits_train,\n",
    "                         validation_data=(x_digits_test,y_digits_test),\n",
    "                         batch_size=128, epochs=10, verbose=1,\n",
    "                         callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WGFTiihd4ouI",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Open Tensorboard\n",
    "\n",
    "# 1) Local installation \n",
    "#  - in your anaconda env, in your project dir, type\"tensorboard --logdir logs\" \n",
    "#  - open browser and goto http://localhost:6006\n",
    "\n",
    "# 2) Google Colab \n",
    "#  - show tensorboard inline\n",
    "%tensorboard --bind_all --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DaXBkWX7veur",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.figure(figsize=(14,6))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='lower right')\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2aGz975Zveux",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# evaluation of the trained model\n",
    "model_digits.evaluate(x_digits_test,y_digits_test)\n",
    "# you get the loss \"categorical_crossentropy\" and the accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8a13np5uveuz",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# prediction of an image with the trained model\n",
    "i=np.random.choice(range(0,len(x_digits_test)))\n",
    "plt.imshow(x_digits_test[i,:,:,0],cmap=\"gray\")\n",
    "pred=model_digits.predict(x_digits_test[i:i+1])\n",
    "print(\"predicted probabilities\",pred)\n",
    "print(\"max probability\",np.max(pred))\n",
    "print(\"predicted label\",np.argmax(pred))\n",
    "print(\"true label\",np.argmax(y_digits_test[i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oyOrTIZjveu3",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In the next cells we calculate the accuracy on the test dataset and look at the confusion matrix. We have an accuracy of around 95% which is already very good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BhA1t9pkveu4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "predict=model_digits.predict(x_digits_test) \n",
    "predict_classes=np.argmax(predict,axis=1)\n",
    "true_classes=np.argmax(y_digits_test,axis=1)\n",
    "confusion_matrix(true_classes,predict_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RJBhLhmDveu6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(np.average(true_classes==predict_classes)) #this should again be accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DJHomXq8vevZ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Now it's your turn\n",
    "\n",
    "Train the same neural network with fewer and more training data. train with 100,1000 and the full training data. Look at the learning curves of each model and evaluate the performace on the test dataset, what do you observe? Play around with the nr of the hidden layer and with the nr of nodes. What do you observe?  \n",
    "*Hint: You might need to train for more than just 10 epochs*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BSjrpJNFveva",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "01_notebook.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "nteract": {
   "version": "0.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
