{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FNvzsBhVypza",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Simpsons characters classification \n",
    "\n",
    "In this notebook we try to classify images of different simpsons characters. The characters are 'abraham_grampa_simpson', 'apu_nahasapeemapetilon', 'bart_simpson', 'charles_montgomery_burns', 'chief_wiggum', 'homer_simpson', 'krusty_the_clown', 'lisa_simpson', 'marge_simpson', 'milhouse_van_houten', 'moe_szyslak', 'ned_flanders', 'principal_skinner' and 'sideshow_bob'.\n",
    "This dataset was preprocessed in an other notebook, it is splitted into a train val and testset and resized into 80x80 pixels and all characters have more than 600 images in total. The whole dataset with the original size can be found here https://www.kaggle.com/alexattia/the-simpsons-characters-dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YCCwPN1J7iqK",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CLeRfAvImGyK",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SBNSW2X-mGyT",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D,GlobalAveragePooling2D,GlobalMaxPooling2D, BatchNormalization\n",
    "from tensorflow.keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uj3NQ0j1vyzb",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "####  Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u8iQqzBg2Yi_"
   },
   "outputs": [],
   "source": [
    "import os,sys,matplotlib\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    %pip install wget\n",
    "    \n",
    "import wget,zipfile\n",
    "\n",
    "if \"labsetup_run\" not in locals() or labsetup_run:\n",
    "\n",
    "    print(\"running setup ...\")\n",
    "\n",
    "    # download data.zip from shared google drive\n",
    "    if not(os.path.isfile(\"data.zip\")): \n",
    "        filename=wget.download(\"https://drive.usercontent.google.com/download?export=download&confirm=yes&id=1dkSV2oL8Ua1SDmzVvtGkyQ0LGQ6VpUIy\",\"data.zip\")\n",
    "    # unpack it\n",
    "    if not(os.path.isdir(\"./data\")):\n",
    "        zf = zipfile.ZipFile(os.path.join(\".\",\"data.zip\"), \"r\")\n",
    "        zf.extractall()\n",
    "                          \n",
    "    # allow \"hot-reloading\" of modules\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "    # needed for inline plots in some contexts\n",
    "    %matplotlib inline\n",
    "\n",
    "    labsetup_run = False  # change to True re-run setup\n",
    "else:\n",
    "    print(\"setup already run.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wPztnK4WHOMv"
   },
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4zmmohbIyd8l",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = \"./data/simpson_data\"\n",
    "Data = pd.read_csv(os.path.join(path, \"Data.csv\"))\n",
    "X_train = np.load(os.path.join(path, \"X_train.npy\"))\n",
    "Y_train = np.load(os.path.join(path, \"Y_train.npy\"))\n",
    "X_val = np.load(os.path.join(path, \"X_val.npy\"))\n",
    "Y_val = np.load(os.path.join(path, \"Y_val.npy\"))\n",
    "X_test = np.load(os.path.join(path, \"X_test.npy\"))\n",
    "Y_test = np.load(os.path.join(path, \"Y_test.npy\"))\n",
    "labels = Data[\"label\"].unique()\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_val.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5sUVgw_G7oN4",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's use the trainset to plot a random image of each character. You can see that the characters are easy recognizable. And all images are the same size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cVMzs5EJyd8u",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "for i in range(0,len(np.unique(np.argmax(Y_train,axis=1)))):\n",
    "    rmd = np.random.choice(np.where(np.argmax(Y_train,axis=1)==i)[0],1)\n",
    "    plt.subplot(4,4,i+1)\n",
    "    img = X_train[rmd]\n",
    "    plt.imshow(img[0,:,:,:])\n",
    "    plt.title(labels[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d8p2GxmZ8H3z",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this cell we plot the label distribution of all sets. You clearly see that the label distribution in all sets is very similar. The biggest class in the trainigset is obviously homer and the smallest class is apu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U2GYK3DVyd8-",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,4))\n",
    "plt.subplot(1,3,1)\n",
    "plt.bar(np.unique(np.argmax(Y_train,axis=1),return_counts=True)[0],np.unique(np.argmax(Y_train,axis=1),return_counts=True)[1]\n",
    "       ,tick_label=labels )\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"train distribution\")\n",
    "plt.subplot(1,3,2)\n",
    "plt.bar(np.unique(np.argmax(Y_val,axis=1),return_counts=True)[0],np.unique(np.argmax(Y_val,axis=1),return_counts=True)[1]\n",
    "       ,tick_label=labels )\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"val distribution\")\n",
    "plt.subplot(1,3,3)\n",
    "plt.bar(np.unique(np.argmax(Y_test,axis=1),return_counts=True)[0],np.unique(np.argmax(Y_test,axis=1),return_counts=True)[1]\n",
    "       ,tick_label=labels )\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"test distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vkq49jpMQcW-",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### CNN\n",
    "\n",
    "Now we normalize the data and use a CNN to classify the images into the right simpson character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tcHL2TwAImOB",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train=np.array(X_train,dtype=\"float32\")\n",
    "X_train=((X_train/255)-0.5)*2\n",
    "\n",
    "X_val=np.array(X_val,dtype=\"float32\")\n",
    "X_val=((X_val/255)-0.5)*2\n",
    "\n",
    "X_test=np.array(X_test,dtype=\"float32\")\n",
    "X_test=((X_test/255)-0.5)*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wLmaVt_uG9ds",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model  =  Sequential()\n",
    "\n",
    "model.add(Conv2D(16,(3,3),activation=\"relu\",padding=\"same\",input_shape=(80,80,3)))\n",
    "model.add(Conv2D(16,(3,3),activation=\"relu\",padding=\"same\"))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(100))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(14))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s2Vrbz-9Mpv7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train, Y_train, epochs=10, validation_data=(X_val, Y_val),verbose=2,batch_size=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Ki1VpnthJx2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='lower right')\n",
    "plt.show()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GP0F6KTfA3sH",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Evaluate\n",
    "Lets check the overall accuracy and the accuracy per class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TqHyB9irnHIM",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "acc = np.average(np.argmax(model.predict(X_test),axis=1)==np.argmax(Y_test,axis=1))\n",
    "res = pd.DataFrame({'Acc' : acc}, index=['CNN'])\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SieNRr-DbzMO",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pred=np.argmax(model.predict(X_test),axis=1)\n",
    "for i in range(0,len(labels)):\n",
    "  print(labels[i],np.average(pred[np.where(np.argmax(Y_test,axis=1)==i)]==i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LOjZirFJW9gJ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Now it's your turn\n",
    "\n",
    "\n",
    "\n",
    "*   Try to fight the overfitting.\n",
    "*   Try to improve the performace on the testset with a different model.  \n",
    "*   *Hints:  You may want to use a deeper CNN, or use transfer learning. Maybe data augmntation could improve the performace or dropout could help to fight the overfitting.*\n",
    "\n",
    "\n",
    "*   Try beat 95% overall accuracy ;-)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "04_3_simpsons_classification_cnn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "nteract": {
   "version": "0.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
